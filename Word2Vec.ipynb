{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3TjX_CX_WRi"
      },
      "source": [
        "### **1. Term Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jvwO5U-_Yhb"
      },
      "source": [
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "          'The greatest glory in living lies not in never falling, but in rising every time we fall.',\n",
        "          'The way to get started is to quit talking and begin doing.',\n",
        "          'If life were predictable it would cease to be life, and be without flavor.',\n",
        "          'Life is what happens when you are busy making other plans'\n",
        "] \n",
        "\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE3KVR_8_a0G"
      },
      "source": [
        "Fungsi fit_transform digunakan untuk mengekstrak kosa kata atau vocabulary dan menghitung kemunculan setiap kata dalam setiap kalimat yang diberikan. Fungsi fit_transform mengembalikan document-term matrix yang dalam hal ini kita simpan dalam variabel X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4969Xnn_Zqd",
        "outputId": "92cfac79-b59e-4371-8c01-029d606d4334"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'are', 'be', 'begin', 'busy', 'but', 'cease', 'doing', 'every', 'fall', 'falling', 'flavor', 'get', 'glory', 'greatest', 'happens', 'if', 'in', 'is', 'it', 'lies', 'life', 'living', 'making', 'never', 'not', 'other', 'plans', 'predictable', 'quit', 'rising', 'started', 'talking', 'the', 'time', 'to', 'way', 'we', 'were', 'what', 'when', 'without', 'would', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVFJ2IPY_e1H"
      },
      "source": [
        "Pada tahap ini melihat dan mengkonversi kumpulan text pada vocabulary menjadi matriks jumlah token (Scikit-learn‚Äôs CountVectorizer is used to convert a collection of text documents to a vector of term/token counts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fAJSWDP_lxR",
        "outputId": "2e4514c8-5066-425c-936b-79185a23e07b"
      },
      "source": [
        "#retrieve the matrix in the numpy form\n",
        "X.toarray()\n",
        "\n",
        "#transforming a new document according to learn vocabulary\n",
        "vectorizer.transform(['A glory.']).toarray()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5BC4VgS_nmg"
      },
      "source": [
        "Fungsi toarray() digunakan untuk menampilkan matrix. Sementara itu, fungsi transform() digunakan untuk merubah dokumen yang diberikan menjadi bentuk document-term matrix. Pada contoh kode tersebut, ‚ÄúA glory.‚Äù diubah menjadi vektor [0,0,0,0,0,0,0,0,0,0,0,0,0,1,‚Ä¶,0] karena kata ‚Äúglory‚Äù memiliki frekuensi 1 dalam dokumen dan berada di index ke-14 darimatriksùëã,sementara ‚Äúa‚Äù tidak ditemui dalam kosakata sehingga tidak ada representasi nilai kemunculannya dalam vektor tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxZ-Cziq_xGS"
      },
      "source": [
        "### **2. Term Frequency ‚Äì Inverse Document Frequency Konversi Kalimat ke N-gram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VceMObBT_rPG",
        "outputId": "eb252a3d-410f-4928-a628-668d516f6754"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# create tf-idf object\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "\n",
        "# Learn the vocabulary and store tf-idf sparse matrix in tfidf\n",
        "tfidf = transformer.fit_transform(X)\n",
        "\n",
        "#retrieveing matrix in numpy form as we did it before\n",
        "tfidf.toarray()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.21080243, 0.        , 0.        , 0.21080243, 0.21080243,\n",
              "        0.21080243, 0.        , 0.        , 0.21080243, 0.21080243,\n",
              "        0.        , 0.        , 0.63240729, 0.        , 0.        ,\n",
              "        0.21080243, 0.        , 0.21080243, 0.        , 0.21080243,\n",
              "        0.21080243, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.21080243, 0.        , 0.        , 0.14957063, 0.21080243,\n",
              "        0.        , 0.        , 0.21080243, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.21871556, 0.        , 0.        , 0.30825419, 0.        ,\n",
              "        0.        , 0.        , 0.30825419, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30825419, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.21871556, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.30825419,\n",
              "        0.        , 0.30825419, 0.30825419, 0.21871556, 0.        ,\n",
              "        0.43743112, 0.30825419, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.18307419, 0.        , 0.51604362, 0.        , 0.        ,\n",
              "        0.        , 0.25802181, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.25802181, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.25802181, 0.        , 0.        , 0.25802181,\n",
              "        0.        , 0.36614837, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.25802181, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.18307419, 0.        , 0.        , 0.25802181, 0.        ,\n",
              "        0.        , 0.25802181, 0.25802181, 0.        ],\n",
              "       [0.        , 0.31611927, 0.        , 0.        , 0.31611927,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.31611927, 0.        , 0.        , 0.22429607, 0.        ,\n",
              "        0.        , 0.22429607, 0.        , 0.31611927, 0.        ,\n",
              "        0.        , 0.31611927, 0.31611927, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.31611927,\n",
              "        0.31611927, 0.        , 0.        , 0.31611927]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bQ8PXH8_7ZL"
      },
      "source": [
        "Transform a count matrix to a normalized tf or tf-idf representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK1a4JAE_2Mu"
      },
      "source": [
        "### **3. Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Eetk_G_1iy",
        "outputId": "03d5feeb-1fbd-4985-91ee-f9de87c2a462"
      },
      "source": [
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "\n",
        "  # importing all necessary modules\n",
        "  from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "  import warnings\n",
        "\n",
        "  warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "  import gensim\n",
        "  from gensim.models import Word2Vec\n",
        "\n",
        "  # Read 'alice.text' file\n",
        "  sample = open(\"./alice_in_wonderland.txt\", \"r\")\n",
        "  s = sample.read()\n",
        "\n",
        "  # Replaces escape character with space\n",
        "  f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "  data = []\n",
        "  # iterate through each sentence in the file\n",
        "  for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "\n",
        "    # tokenize the_sentece into words\n",
        "    for j in word_tokenize(i):\n",
        "      temp.append(j.lower())\n",
        "\n",
        "    data.append(temp)\n",
        "\n",
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpbktVx_-0S"
      },
      "source": [
        "Pada pemodelan CBOW tersebut terdapat 3 hyperpameter yaitu,\n",
        "\n",
        "a. min_count : jumlah kata minimum yang perlu dipertimbangkan saat melatih model\n",
        "\n",
        "b. size : jumlah dari dimensi embedding\n",
        "\n",
        "c. window: jarak maksimum antara kata target dan kata disekitar kata target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GL9eYTQABMS",
        "outputId": "2afe0a0d-1b69-4162-bc62-2195ab070ce9"
      },
      "source": [
        "# Print results\n",
        "print(\"Consine similarity between 'alice' \" + \"and 'wonderland' - CBOW: \",\n",
        "      model1.similarity('alice', 'wonderland'))\n",
        "\n",
        "print(\"Consine similarity between 'alice' \" + \"and 'machines' - CBOW: \",\n",
        "      model1.similarity('alice', 'machines'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consine similarity between 'alice' and 'wonderland' - CBOW:  0.9975647\n",
            "Consine similarity between 'alice' and 'machines' - CBOW:  0.9896047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3rWhUw6ACig"
      },
      "source": [
        "Word2vec dapat digunakan untuk menghitung kesamaan antara dua kata dalam kosakata dengan me-manggil fungsi model.similarity ()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmkGmDm0AFDk",
        "outputId": "700a0215-82b9-4f98-baf9-9426cd3ab566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create skip Gram model\n",
        "model2= gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1 )\n",
        "\n",
        "# Print results\n",
        "print(\"Consine similarity between 'alice' \" + \"and 'wonderland' - Skip Gram: \",\n",
        "      model2.similarity('alice', 'wonderland'))\n",
        "\n",
        "print(\"Consine similarity between 'alice' \" + \"and 'machines' - Skip Gram: \",\n",
        "      model2.similarity('alice', 'machines'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consine similarity between 'alice' and 'wonderland' - Skip Gram:  0.94407004\n",
            "Consine similarity between 'alice' and 'machines' - Skip Gram:  0.9343238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq6YNNSnAHGW"
      },
      "source": [
        "Untuk membuat model skip-gram, kita hanya perlu menambahkan satu hyperparameter yaitu ‚Äòsg‚Äô pada fungsi Word2Vec."
      ]
    }
  ]
}